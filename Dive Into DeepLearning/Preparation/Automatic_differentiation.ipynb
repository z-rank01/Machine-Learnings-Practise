{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar to Vector differentiation:\n",
      "tensor([0., 1., 2., 3.])\n",
      "None\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "\n",
      "Vector to Vector differentiation:\n",
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentiation pipeline\n",
    "# 1. record data which needs computation of gradients\n",
    "# 2. framework will create computational graph according to variaties, for example\n",
    "#    x = tensor(...), y = operation(x), now we have nodes recording the x, y and their function.\n",
    "# 3. backward for computation of gradients. graph will backward traverse every \n",
    "#    results and parameters in grapgh and accordingly compute gradients on every correalted node\n",
    "\n",
    "import torch\n",
    "\n",
    "# Scalar to Vector\n",
    "# ----------------\n",
    "# prepare x for function y\n",
    "print('Scalar to Vector differentiation:')\n",
    "x = torch.arange(4.0)\n",
    "print(x)\n",
    "\n",
    "# tell api to start record\n",
    "x.requires_grad_(True)\n",
    "print(x.grad)  # now it is None, beacuse there is no function at all\n",
    "\n",
    "# make function\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(y)\n",
    "\n",
    "# backward for gradients\n",
    "y.backward()\n",
    "print(x.grad)  # now when we need to compute partial differentiation of x from y, we get results\n",
    "\n",
    "# clean up gradients if we done gradients or pytorch API will keep recording every variaties about x\n",
    "x.grad.zero_()\n",
    "\n",
    "# Vector to Vector\n",
    "# ---------------\n",
    "print('\\nVector to Vector differentiation:')\n",
    "y = x * x  # this operation will return a vector to y, whose function are Vector to Vector type\n",
    "\n",
    "# sum() BEFORE backward() !!\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  8., 27.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 1., 4., 9.])\n",
      "tensor([0., 1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "# detach\n",
    "# ------\n",
    "# if we want to compute a intermediate variable like y in z(y), y(x), x as a constant, \n",
    "# we could use .detach() function to extract result of y(x) as a constant u, so that when we compute\n",
    "# D(z(x)) we will compute z(u * x) without computing y(x) reuslt for z(y(x))\n",
    "\n",
    "x.grad.zero_()\n",
    "y = x * x        # we know that y = [0, 1, 4, 9]\n",
    "u = y.detach()   # we use this vector results as a constant reducing computing of BP\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "print(z)         # originally get z from z(x) = x * x * x, but now we get z from z(x) = u[0, 1, 4, 9].dot(x)\n",
    "print(x.grad)\n",
    "print(u)  # we could see that u are consider as a constant not \n",
    "          # the original y function so it exerts nothing to gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datawhale",
   "language": "python",
   "name": "datawhale"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
