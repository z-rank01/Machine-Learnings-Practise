{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opinion Mining / Sentiment Analysis\n",
    "# technique: bag-of-word\n",
    "# 1. tokenize every documents: raw documents are converted into individual words/roots set.\n",
    "# 2. create feature matrix depending on frequency of tokens(words) occurency in particular documents\n",
    "\n",
    "# load movie comments dataset\n",
    "# import pyprind\n",
    "import pandas as pd\n",
    "# import os\n",
    "\n",
    "# basepath = 'aclImdb'\n",
    "\n",
    "# labels = {'pos':1, 'neg':0}\n",
    "# pbar = pyprind.ProgBar(50000)\n",
    "# df = pd.DataFrame()\n",
    "# for s in ('test', 'train'):\n",
    "#     for l in ('pos', 'neg'):\n",
    "#         path = os.path.join(basepath, s, l)\n",
    "#         for file in sorted(os.listdir(path)):\n",
    "#             with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "#                 txt = infile.read()\n",
    "#             df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "#             pbar.update()\n",
    "# df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1  OK... so... I really like Kris Kristofferson a...          0\n",
      "2  ***SPOILER*** Do not read this, if you think a...          0\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# store comments as csv file\n",
    "import numpy as np\n",
    "# np.random.seed(0)\n",
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "print(df.head(3))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is seven title brazil not available\n"
     ]
    }
   ],
   "source": [
    "# clean up dataset\n",
    "\n",
    "# 1. remove html tokens and non-word chars\n",
    "# 2. store emoticons\n",
    "# 3. lower the word \n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "print(preprocessor(df.loc[0, 'review'][-50:]))\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 1974 the teenager martha moxley maggie grace moves to the high class area of belle haven greenwic\n",
      "Simply Split:  ['in', '1974', 'the', 'teenager', 'martha', 'moxley', 'maggie', 'grace', 'moves', 'to', 'the', 'high', 'class', 'area', 'of', 'belle', 'haven', 'greenwic']\n",
      "Porter Stemmer:  ['in', '1974', 'the', 'teenag', 'martha', 'moxley', 'maggi', 'grace', 'move', 'to', 'the', 'high', 'class', 'area', 'of', 'bell', 'haven', 'greenwic']\n",
      "SnowBall Stemmer:  ['in', '1974', 'the', 'teenag', 'martha', 'moxley', 'maggi', 'grace', 'move', 'to', 'the', 'high', 'class', 'area', 'of', 'bell', 'haven', 'greenwic']\n",
      "lancaster Stemmer:  ['in', '1974', 'the', 'teen', 'marth', 'moxley', 'maggy', 'grac', 'mov', 'to', 'the', 'high', 'class', 'are', 'of', 'bel', 'hav', 'greenw']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\12923\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# bag-of-word\n",
    "\n",
    "# 1. tokenize \n",
    "# ------------\n",
    "# create vocabulary of documents('review' in this case)\n",
    "\n",
    "text = df.loc[0, 'review'][:100]\n",
    "print(text)\n",
    "\n",
    "# a. simply split every work according to 'space'\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "print('Simply Split: ', tokenizer(text))\n",
    "\n",
    "# b. word stemming: convert word into root\n",
    "# porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "print('Porter Stemmer: ', tokenizer_porter(text))\n",
    "\n",
    "# snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "def tokenizer_snowball(text):\n",
    "    return [snowball.stem(word) for word in text.split()]\n",
    "print('SnowBall Stemmer: ', tokenizer_snowball(text))\n",
    "\n",
    "# lancaster stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "def tokenizer_lancaster(text):\n",
    "    return [lancaster.stem(word) for word in text.split()]\n",
    "print('lancaster Stemmer: ', tokenizer_lancaster(text))\n",
    "\n",
    "# stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ths': 7, 'sun': 4, 'is': 1, 'shining': 3, 'the': 6, 'weather': 9, 'sweet': 5, 'and': 0, 'one': 2, 'two': 8}\n",
      "[[0 1 0 1 1 0 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 1]\n",
      " [0 2 0 1 1 1 2 0 0 1]\n",
      " [2 1 2 0 0 0 0 0 1 0]]\n",
      "[[0.    0.329 0.    0.497 0.497 0.    0.    0.631 0.    0.   ]\n",
      " [0.    0.357 0.    0.    0.    0.539 0.539 0.    0.    0.539]\n",
      " [0.    0.424 0.    0.32  0.32  0.32  0.64  0.    0.    0.32 ]\n",
      " [0.657 0.171 0.657 0.    0.    0.    0.    0.    0.328 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# 2. tf, tf-idf\n",
    "# -------------------------------------------------------\n",
    "# transform words/vocabularies into numerical features\n",
    "\n",
    "# a. tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array(['Ths sun is shining', \n",
    "                 'The weather is sweet', \n",
    "                 'The sun is shining, the weather is sweet', \n",
    "                 'and one and one is two'])\n",
    "tf = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(tf.toarray())\n",
    "\n",
    "# b. tf-idf, weighted tf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2', \n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=3)\n",
    "print(tfidf.fit_transform(tf).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in 1974 the teenager martha moxley maggie grace moves to the high class area of belle haven greenwich connecticut on the mischief night eve of halloween she was murdered in the backyard of her house and her murder remained unsolved twenty two years later the writer mark fuhrman christopher meloni who is a former la detective that has fallen in disgrace for perjury in o j simpson trial and moved to idaho decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals squirm and do not welcome them but with the support of the retired detective steve carroll robert forster that was in charge of the investigation in the 70 s they discover the criminal and a net of power and money to cover the murder murder in greenwich is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the dramatization my vote is seven title brazil not available'\n",
      " 'ok so i really like kris kristofferson and his usual easy going delivery of lines in his movies age has helped him with his soft spoken low energy style and he will steal a scene effortlessly but disappearance is his misstep holy moly this was a bad movie i must give kudos to the cinematography and and the actors including kris for trying their darndest to make sense from this goofy confusing story none of it made sense and kris probably didn t understand it either and he was just going through the motions hoping someone would come up to him and tell him what it was all about i don t care that everyone on this movie was doing out of love for the project or some such nonsense i ve seen low budget movies that had a plot for goodness sake this had none zilcho nada zippo empty of reason a complete waste of good talent scenery and celluloid i rented this piece of garbage for a buck and i want my money back i want my 2 hours back i invested on this grade f waste of my time don t watch this movie or waste 1 minute of your valuable time while passing through a room where it s playing or even open up the case that is holding the dvd believe me you ll thank me for the advice '\n",
      " ' spoiler do not read this if you think about watching that movie although it would be a waste of time by the way the plot is so predictable that it does not make any difference if you read this or not anyway if you are wondering whether to see coyote ugly or not don t it s not worth either the money for the ticket or the vhs dvd a typical chick feel good flick one could say the plot itself is as shallow as it can be a ridiculous and uncritical version of the american dream the young good looking girl from a small town becoming a big success in new york the few desperate attempts of giving the movie any depth fail such as the tragic accident of the father the difficulties of violet s relationship with her boyfriend and so on mcnally director tries to arouse the audience s pity and sadness put does not have any chance to succeed in this attempt due to the bad script and the shallow acting especially piper perabo completely fails in convincing one of jersey s fear of singing in front of an audience the only good and quite funny thing about coyote ugly is john goodman who represents the small ray of hope of this movie i was very astonished that jerry bruckheimer produced this movie first gone in 60 seconds and now this what happened to great movies like the rock and con air that was true bruckheimer stuff if you are looking for a superficial movie with good looking women just to have a relaxed evening you should better go and see charlie s angels it s much more funny entertaining and self ironic instead of this flick two thumbs down 3 out of 10 '\n",
      " ...\n",
      " 'i have seen several comments here about brando using a southern accent some of which felt it was a mistake when this movie was made racism and discrimination were very strong in the south the jim crow laws were still in effect civil rights was in it s infancy could this have possibly been a subtle social commentary a southern man in love with a woman of another race the same way mash was a subtle criticism of the viet nam war any thoughts another comment was made about myoshi umeki appearing cold anyone who has been in japan would understand the japanese people at least in my experience did not tend to show emotion in front of strangers there were strict social rules especially for men meeting single women americans in japan were totally foreign to this culture and the blunt attempts to meet women were shocking to the ladies one trait of the japanese was to smile when embarrassed or uncomfortable which many american servicemen took as a sign that their advances were welcomed also remember that at the time represented in the movie japan had just been defeated and the occupying forces were treated with reluctant acceptance i think myoshi umeki gave a very credible performance of what her situation would have been watching her interaction with the american actors brought back several memories of my own experiences in the country i was able to meet a pair of lovely young ladies who after i convinced them i was not the typical american male taught me their language and their culture during my time in their country '\n",
      " 'i liked this film very much the story jumps back and forth quite a bit and is not easy to follow there is no resolution to the story whatsoever and you are left to wonder what really happened since i like that sort of film i enjoyed this i especially like the dating scenes between the boys and i was drawn into their lives and of course any film with a naked staphane rideau will get a couple of extra points ;)'\n",
      " 'there s a part of me that would like to give this movie a high rating considering that it was made in 1953 this is a very courageous movie about transvestites tackling the issue fairly seriously and sympathetically and offering the viewer a lot of information on the subject and trying very hard not to stereotype the movie clearly makes the point that transvestites are not homosexuals and that aside from wearing women s clothing they lead a relatively normal life it deals with the pain of not being accepted in society the plot revolves around a police officer lyle talbot desperately trying to understand the issue because of the recent suicide of a transvestite so you have to give everyone involved with this movie credit for taking on such a controversial in the context of 1953 subject having said all that i m also sorry to say that this movie is absolutely dreadful in trying to portray glen glenda s edward d wood pain the movie falls into silly and at times surprisingly again given the era sensual fantasies that make the story very hard to follow the acting is wooden at best none of the dialogue comes across as real the actors look and sound like people reading speeches written by others and worst of all there was no point to having bela lugosi in this movie this was another of the increasingly embarrassing roles this poor man took on in the latter stages of his career pull the strings pull the strings poor lugosi s character called the spirit in the credits but really coming across as more of a mad scientist kept crying and nothing he did really seemed to have much connection with the rest of the movie for artistic merit the movie doesn t really deserve much more than 1 10 however for the courage involved in just putting it out i ll give it a 3 10 ']\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m lr_tfidf \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mvect\u001b[39m\u001b[39m'\u001b[39m, tfidf), (\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m, LogisticRegression(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, solver\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m))])\n\u001b[0;32m     36\u001b[0m gs \u001b[39m=\u001b[39m GridSearchCV(lr_tfidf, para_grid, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m gs\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\12923\\.conda\\envs\\env_datascience\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training model with bag-of-word\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False, \n",
    "                        preprocessor=None)\n",
    "para_grid = [{\n",
    "              'vect__ngram_range':[(1, 1)], \n",
    "              'vect__stop_words':[stop, None], \n",
    "              'vect__tokenizer':[tokenizer, tokenizer_porter], \n",
    "              'clf__penalty':['l1', 'l2'], \n",
    "              'clf__C':[1.0, 10.0, 100.0]\n",
    "              }, \n",
    "             {\n",
    "              'vect__ngram_range':[(1, 1)], \n",
    "              'vect__stop_words':[stop, None], \n",
    "              'vect__tokenizer':[tokenizer, tokenizer_porter], \n",
    "              'vect__use_idf':[False], \n",
    "              'vect__norm':[None], \n",
    "              'clf__penalty':['l1', 'l2'], \n",
    "              'clf__C':[1.0, 10.0, 100.0]\n",
    "             }\n",
    "            ]\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0, solver='liblinear'))])\n",
    "gs = GridSearchCV(lr_tfidf, para_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)  # briefly 24m32.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter Set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001F7F445DBD0>} \n",
      "CV Accuracy: 0.897\n",
      "Test Accuracy: 0.899 \n"
     ]
    }
   ],
   "source": [
    "print('Best parameter Set: %s ' % gs.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs.best_score_)\n",
    "clf = gs.best_estimator_\n",
    "print('Test Accuracy: %.3f ' % clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_datascience",
   "language": "python",
   "name": "env_datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
